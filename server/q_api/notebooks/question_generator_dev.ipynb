{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcf4a2b-b4c9-422b-a59b-5fdbdf399840",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69202d1-e947-45f3-ae07-77345cf2c97f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf369e8-ebce-4029-a62d-7c99ef321410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package punkt to /Users/amc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flask import Flask, request, render_template, url_for\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import os \n",
    "import re\n",
    "import json \n",
    "import linecache\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import nltk \n",
    "from nltk import tokenize\n",
    "import ssl\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "#Local storage of the conversation data - will be deprecated once the database is in place\n",
    "storage = []\n",
    "\n",
    "starters = [\"What topics would you like to talk about?\", \"What are your hobbies?\", \"Where did you study?\"]\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294125c-8ea4-4024-bf6c-21f869e26aa4",
   "metadata": {},
   "source": [
    "## Q Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd9b59f-7826-492a-b7fd-fd6b1922984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc7242f-e0c9-40d7-af66-674aa862f96f",
   "metadata": {},
   "source": [
    "The function below isn't used for now..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "190e334d-ed97-43c8-a37a-3bb4e9e50c5a",
   "metadata": {},
   "source": [
    "def generateNextQ():\n",
    "\n",
    "    # UNCOMMENT AFTER INTEGRATION WITH BACKEND\n",
    "\n",
    "    body_unicode = request.data.decode('utf-8')\n",
    "    body = json.loads(body_unicode)\n",
    "\n",
    "    print(\"Received body\", body)\n",
    "    text=body['qa_pair']\n",
    "\n",
    "    storage.append(text)\n",
    "\n",
    "    if len(starters) > 0: \n",
    "        print(\"SENDING STARTER\")\n",
    "        return {\"q\":starters.pop()}\n",
    "\n",
    "    else: \n",
    "\n",
    "        text = \" \".join(storage[-2:])\n",
    "        q = generator(text, num_return_sequences=3,max_length=50+len(text))\n",
    "\n",
    "        #all generated examples \n",
    "        allGenerations = \"\"\n",
    "        for i in range(3):\n",
    "            allGenerations = allGenerations +\" \"+ q[i]['generated_text'][len(text)-4:]\n",
    "        \n",
    "        #Separating all the sentences... \n",
    "        sentenceList = nltk.tokenize.sent_tokenize(allGenerations)\n",
    "\n",
    "        #Filter out questions \n",
    "        questionsList = []\n",
    "        for sentence in sentenceList :\n",
    "            if \"?\" in sentence:\n",
    "                questionsList.append(sentence.strip(\"\\n\").strip(\"\\\\\").strip('\"'))\n",
    "\n",
    "        #Bert evaluation\n",
    "        bert_filtered_qs = []\n",
    "        for sentence in questionsList:\n",
    "            encoding = tokenizer(\" \".join(storage[-3:]), sentence, return_tensors='pt')\n",
    "            outputs = model(**encoding)\n",
    "            logits = outputs.logits\n",
    "            bert_filtered_qs.append((logits[0,0].item(), sentence))\n",
    "\n",
    "\n",
    "        bert_filtered_qs.sort(key=lambda tup: tup[0])\n",
    "\n",
    "\n",
    "        print(bert_filtered_qs)\n",
    "\n",
    "        return {\"q\":bert_filtered_qs[-1][1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3711b-36c7-42b3-bc90-f282ced3541d",
   "metadata": {},
   "source": [
    "## Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65dc5d89-fdbc-4f99-ae69-33eafd3e08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_trial_generateNextQ(text):\n",
    "# text = \"What's your name? My name is Alberto.\"\n",
    "\n",
    "    storage.append(text)\n",
    "\n",
    "    if len(starters) > 0: \n",
    "        print(\"SENDING STARTER\")\n",
    "        return {\"q\":starters.pop()}\n",
    "\n",
    "    else: \n",
    "\n",
    "        text = \" \".join(storage[-2:])\n",
    "        q = generator(text, num_return_sequences=3,max_length=50+len(text))\n",
    "\n",
    "        #all generated examples \n",
    "        allGenerations = \"\"\n",
    "        for i in range(3):\n",
    "            allGenerations = allGenerations +\" \"+ q[i]['generated_text'][len(text)-4:]\n",
    "\n",
    "        #Separating all the sentences... \n",
    "        sentenceList = nltk.tokenize.sent_tokenize(allGenerations)\n",
    "\n",
    "        #Filter out questions \n",
    "        questionsList = []\n",
    "        for sentence in sentenceList :\n",
    "            if \"?\" in sentence:\n",
    "                questionsList.append(sentence.strip(\"\\n\").strip(\"\\\\\").strip('\"'))\n",
    "\n",
    "        #Bert evaluation\n",
    "        bert_filtered_qs = []\n",
    "        for sentence in questionsList:\n",
    "            encoding = tokenizer(\" \".join(storage[-3:]), sentence, return_tensors='pt')\n",
    "            outputs = model(**encoding)\n",
    "            logits = outputs.logits\n",
    "            bert_filtered_qs.append((logits[0,0].item(), sentence))\n",
    "\n",
    "\n",
    "        bert_filtered_qs.sort(key=lambda tup: tup[0])\n",
    "\n",
    "\n",
    "        print(bert_filtered_qs)\n",
    "\n",
    "        # return {\"q\":bert_filtered_qs[-1][1]}\n",
    "        print({\"q\":bert_filtered_qs[-1][1]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02879b-1c88-469d-aca3-e43a332f5400",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = []\n",
    "\n",
    "starters = [\"What topics would you like to talk about?\", \"What are your hobbies?\", \"Where did you study?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f40aa58f-1975-474a-b4aa-e998a1b714ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5.492365837097168, \"Where's your favorite place on Earth and why do you think it is it would be the most romantic place you could love to be alive?\"), (5.694579601287842, 'Any advice that could help you start the process of your goal set?'), (5.900996208190918, \"What would that help you do for the rest of the life you've been in the game?\"), (6.091255187988281, 'Do you have other training options in mind?'), (6.170355796813965, 'If you do it, what can you bring to the gym that would help you do that?')]\n",
      "{'q': 'If you do it, what can you bring to the gym that would help you do that?'}\n",
      "\n",
      "#############\n",
      " [] \n",
      "#############\n",
      " ['What topics would you like to talk about? Tesla cars, philosophy, food and movies.', 'Where did you study? In Italy, in Milan.', \"What's your name? My name is Alberto.\", 'Nice to meet you. Nice to meey you too.', \"But don't you think I'm a spy? I hope not. In any case, I recorded only answers that I'm okay sharing publicly!\", \"Is there a chance I'd be too serious? I can only answer questions about me, sorry.\", \"Is there a chance I'd be too serious? I can only answer questions about me, sorry.\", 'What food you like? I like a good steak with fries.', 'What is the right way to do that? Medium-rare, and the fries double fried!', \"Are you concerned about meat's impact on CO2? I eat meat rarely. If the world had a truly balanced diete, we would pollute much less, and stay healthier.\", \"If what you are going to eat can help you lose weight, what if you can actually do it on your own? I exercise daily, minimum half hour. Diet only won't get you there as well as you cannot workout a bad diet.\", \"What kind of exercise do you do? I use Apple Fitness+, it's great as it offer a variety of workouts. I tend to alternate cardio days with strength training.\", \"Who's your favorite trainer? Kim, I love her British accent and fun vibe!\", \"Where could you possibly go next? I'm quite happy where I am.\", \"What's the best thing you've learned about doing and training as an American? I'm not an American, but I've learned I have the inner willpower to do what it takes to build a better version of myself.\"]\n"
     ]
    }
   ],
   "source": [
    "nb_trial_generateNextQ(\"What's the best thing you've learned about doing and training as an American? I'm not an American, but I've learned I have the inner willpower to do what it takes to build a better version of myself.\")\n",
    "\n",
    "print(\"\\n#############\\n\", \n",
    "      starters, \n",
    "      \"\\n#############\\n\", \n",
    "      storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a1aff2-3367-440b-9ac5-65a447ea5115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcf4a2b-b4c9-422b-a59b-5fdbdf399840",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69202d1-e947-45f3-ae07-77345cf2c97f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8cf369e8-ebce-4029-a62d-7c99ef321410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package punkt to /Users/amc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from flask import Flask, request, render_template, url_for\n",
    "# import os\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import json \n",
    "import linecache\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import nltk \n",
    "from nltk import tokenize\n",
    "import ssl\n",
    "import torch\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "# #Local storage of the conversation data - will be deprecated once the database is in place\n",
    "# storage = []\n",
    "\n",
    "# starters = [\"What topics would you like to talk about?\", \"What are your hobbies?\", \"Where did you study?\"]\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294125c-8ea4-4024-bf6c-21f869e26aa4",
   "metadata": {},
   "source": [
    "## Q Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd9b59f-7826-492a-b7fd-fd6b1922984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc7242f-e0c9-40d7-af66-674aa862f96f",
   "metadata": {},
   "source": [
    "The function below isn't used for now..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "190e334d-ed97-43c8-a37a-3bb4e9e50c5a",
   "metadata": {},
   "source": [
    "def generateNextQ():\n",
    "\n",
    "    # UNCOMMENT AFTER INTEGRATION WITH BACKEND\n",
    "\n",
    "    body_unicode = request.data.decode('utf-8')\n",
    "    body = json.loads(body_unicode)\n",
    "\n",
    "    print(\"Received body\", body)\n",
    "    text=body['qa_pair']\n",
    "\n",
    "    storage.append(text)\n",
    "\n",
    "    if len(starters) > 0: \n",
    "        print(\"SENDING STARTER\")\n",
    "        return {\"q\":starters.pop()}\n",
    "\n",
    "    else: \n",
    "\n",
    "        text = \" \".join(storage[-2:])\n",
    "        q = generator(text, num_return_sequences=3,max_length=50+len(text))\n",
    "\n",
    "        #all generated examples \n",
    "        allGenerations = \"\"\n",
    "        for i in range(3):\n",
    "            allGenerations = allGenerations +\" \"+ q[i]['generated_text'][len(text)-4:]\n",
    "        \n",
    "        #Separating all the sentences... \n",
    "        sentenceList = nltk.tokenize.sent_tokenize(allGenerations)\n",
    "\n",
    "        #Filter out questions \n",
    "        questionsList = []\n",
    "        for sentence in sentenceList :\n",
    "            if \"?\" in sentence:\n",
    "                questionsList.append(sentence.strip(\"\\n\").strip(\"\\\\\").strip('\"'))\n",
    "\n",
    "        #Bert evaluation\n",
    "        bert_filtered_qs = []\n",
    "        for sentence in questionsList:\n",
    "            encoding = tokenizer(\" \".join(storage[-3:]), sentence, return_tensors='pt')\n",
    "            outputs = model(**encoding)\n",
    "            logits = outputs.logits\n",
    "            bert_filtered_qs.append((logits[0,0].item(), sentence))\n",
    "\n",
    "\n",
    "        bert_filtered_qs.sort(key=lambda tup: tup[0])\n",
    "\n",
    "\n",
    "        print(bert_filtered_qs)\n",
    "\n",
    "        return {\"q\":bert_filtered_qs[-1][1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3711b-36c7-42b3-bc90-f282ced3541d",
   "metadata": {},
   "source": [
    "## Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ffa40950-6747-4027-95fa-123e528ba324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db\n",
    "from sqlalchemy.sql import text as QueryText\n",
    "\n",
    "SQL_URL = \"mysql+pymysql://root:ReallyComplicatedPassword@localhost:3307/toia\"\n",
    "\n",
    "ENGINE = db.create_engine(SQL_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "65dc5d89-fdbc-4f99-ae69-33eafd3e08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(new_q, new_a, api):\n",
    "    \n",
    "    statement = QueryText(\"\"\"\n",
    "        SELECT questions.question, video.answer AS latest_question_answer \n",
    "        FROM video\n",
    "        INNER JOIN videos_questions_streams\n",
    "        ON videos_questions_streams.id_video = video.id_video\n",
    "        INNER JOIN questions\n",
    "        ON questions.id = videos_questions_streams.id_question\n",
    "        WHERE toia_id=1 \n",
    "        AND questions.trigger_suggester = 1\n",
    "        ORDER BY video.idx DESC LIMIT 1;\n",
    "        \"\"\")\n",
    "    CONNECTION = ENGINE.connect()  #Need to refresh connection\n",
    "    result_proxy = CONNECTION.execute(statement)\n",
    "    result_set = result_proxy.fetchall()\n",
    "    \n",
    "    if api == \"GPT-2\":      \n",
    "        prompt = new_q + \" \" + new_a\n",
    "        if len(result_set) > 0:\n",
    "            prompt = \"\"\"{} {} {}\"\"\".format(\n",
    "                result_set[0][0],\n",
    "                result_set[0][1],\n",
    "                prompt)\n",
    "        \n",
    "    elif api == \"GPT-3\":\n",
    "        prompt = \"\"\"Suggest five plausible questions.\n",
    "{}\n",
    "Q: {}\n",
    "A: {}\n",
    "Possible questions:\n",
    "\"\"\"\n",
    "        if len(result_set) == 0:\n",
    "            prompt = prompt.format(\"\", new_q, new_a)\n",
    "        else:\n",
    "            prompt = prompt.format(\n",
    "\"\"\"\n",
    "Q: {}\n",
    "A: {}\"\"\".format(result_set[0][0], result_set[0][1]),\n",
    "                new_q, new_a)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def nb_trial_generateNextQ(new_q, new_a, api, n_suggestions=5):\n",
    "    \n",
    "    prompt = generate_prompt(new_q=new_q, new_a=new_a, api=api)\n",
    "    \n",
    "    if api == \"GPT-2\":\n",
    "        q = generator(prompt, \n",
    "                  num_return_sequences=n_suggestions, \n",
    "                  max_length=50 + len(prompt))\n",
    "\n",
    "        #all generated examples \n",
    "        allGenerations = \"\"\n",
    "        for i in range(n_suggestions):\n",
    "            allGenerations = allGenerations + \" \" + q[i]['generated_text'][len(text) - 4:]\n",
    "\n",
    "        #Separating all the sentences... \n",
    "        sentenceList = nltk.tokenize.sent_tokenize(allGenerations)\n",
    "\n",
    "        #Filter out questions \n",
    "        questionsList = []\n",
    "        for sentence in sentenceList :\n",
    "            if \"?\" in sentence:\n",
    "                questionsList.append(sentence.strip(\"\\n\").strip(\"\\\\\").strip('\"'))\n",
    "\n",
    "        #Bert evaluation\n",
    "        bert_filtered_qs = []\n",
    "        for sentence in questionsList:\n",
    "            encoding = tokenizer(\" \".join(storage[-3:]), sentence, return_tensors='pt')\n",
    "            outputs = model(**encoding)\n",
    "            logits = outputs.logits\n",
    "            bert_filtered_qs.append((logits[0,0].item(), sentence))\n",
    "\n",
    "        bert_filtered_qs.sort(key=lambda tup: tup[0], reverse=True)\n",
    "        # Update number of suggestions in case there are less than n_suggestion questions identified\n",
    "        n_suggestions = min(len(bert_filtered_qs) - 1, n_suggestions)\n",
    "        suggestions = [bert_filtered_qs[i][1] for i in range(n_suggestions) if bert_filtered_qs[i][1] != bert_filtered_qs[i + 1][1]]     \n",
    "        \n",
    "    elif api == \"GPT-3\":      \n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-001\",\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=250\n",
    "        )\n",
    "        \n",
    "        generation = response.choices[0]['text']\n",
    "        # remove new lines, numbered lists, or - lists.\n",
    "        generation = re.sub(r'\\n+[0-9]+.|\\n+[0-9]+)|\\n+-|\\n+ -|\\n\\n', \"\", generation)\n",
    "        # split sentences into list\n",
    "        suggestions = nltk.tokenize.sent_tokenize(generation)\n",
    "        # strip trailing white spaces\n",
    "        suggestions = [suggestion.strip() for suggestion in suggestions]\n",
    "        \n",
    "    print(prompt)\n",
    "    print(suggestions)\n",
    "    \n",
    "    return suggestions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e97f8b82-bd19-4e8b-bf45-52cfc3429aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where did you take your degrees from? I've got a PhD at NYU. What topics would you like to talk about? Italian food, my job and technology.\n",
      "['How does your university help me pay the rent?', 'When will you get into tech?', 'When do you start your programming program?', 'If I had the opportunity to start with a couple tech jobs, what would you do?', 'What do you like to do your programming like?']\n"
     ]
    }
   ],
   "source": [
    "suggestions = nb_trial_generateNextQ(\n",
    "    new_q=\"What topics would you like to talk about?\", \n",
    "    new_a=\"Italian food, my job and technology.\", \n",
    "    api=\"GPT-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ca7e8516-de08-43e8-b8bb-cae3eaf4e6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-4lZTpiTnet7sDOkeYMRZ2BDaqozwv at 0x159946cc0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\n1. How do you like working alone?\\n2. What's your favorite thing about your job?\\n3. What's been the biggest challenge for you so far?\\n4. What do you think makes your company unique?\\n5. Why did you choose to study at NYU?\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1647243149,\n",
       "  \"id\": \"cmpl-4lZTpiTnet7sDOkeYMRZ2BDaqozwv\",\n",
       "  \"model\": \"text-davinci:001\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(\n",
    "    engine=\"text-davinci-001\",\n",
    "    prompt=\"\"\"\n",
    "Suggest five plausible questions.\n",
    "\n",
    "Q: Where did you take your degrees from?\n",
    "A: I've got a PhD at NYU.\n",
    "Q: What's your biggest challenge here?\n",
    "A: Working completely alone.\n",
    "Possible questions:\"\"\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=250\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

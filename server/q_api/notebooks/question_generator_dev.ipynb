{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcf4a2b-b4c9-422b-a59b-5fdbdf399840",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69202d1-e947-45f3-ae07-77345cf2c97f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf369e8-ebce-4029-a62d-7c99ef321410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package punkt to /Users/amc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from flask import Flask, request, render_template, url_for\n",
    "# import os\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import json \n",
    "import linecache\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import nltk \n",
    "from nltk import tokenize\n",
    "import ssl\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "# #Local storage of the conversation data - will be deprecated once the database is in place\n",
    "# storage = []\n",
    "\n",
    "# starters = [\"What topics would you like to talk about?\", \"What are your hobbies?\", \"Where did you study?\"]\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294125c-8ea4-4024-bf6c-21f869e26aa4",
   "metadata": {},
   "source": [
    "## Q Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd9b59f-7826-492a-b7fd-fd6b1922984e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09aa3ede5c9349449238348f3639d50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9ceb99ac074537aeeeac24eb475c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT2Model:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50257, 1536]) from checkpoint, the shape in current model is torch.Size([50264, 1536]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDeniskin/gpt3_medium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/q_api-Olle6l79/lib/python3.8/site-packages/transformers/pipelines.py:3231\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, framework, revision, use_fast, **kwargs)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3228\u001b[0m     \u001b[38;5;66;03m# At that point framework might still be undetermined\u001b[39;00m\n\u001b[1;32m   3229\u001b[0m     model \u001b[38;5;241m=\u001b[39m get_default_model(targeted_task, framework, task_options)\n\u001b[0;32m-> 3231\u001b[0m framework \u001b[38;5;241m=\u001b[39m framework \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mget_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3233\u001b[0m task_class, model_class \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m], targeted_task[framework]\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;66;03m# Try to infer tokenizer from model or config name (if provided as str)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/q_api-Olle6l79/lib/python3.8/site-packages/transformers/pipelines.py:107\u001b[0m, in \u001b[0;36mget_framework\u001b[0;34m(model, revision)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n\u001b[0;32m--> 107\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m    109\u001b[0m         model \u001b[38;5;241m=\u001b[39m TFAutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/q_api-Olle6l79/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:693\u001b[0m, in \u001b[0;36mAutoModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    689\u001b[0m         pretrained_model_name_or_path, return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    690\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m MODEL_MAPPING\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    699\u001b[0m         config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m MODEL_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    701\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/q_api-Olle6l79/lib/python3.8/site-packages/transformers/modeling_utils.py:1154\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1149\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll the weights of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were initialized from the model checkpoint at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1150\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf your task is similar to the task the model of the checkpoint was trained on, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1151\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can already use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for predictions without further training.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1152\u001b[0m         )\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1156\u001b[0m                 model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   1157\u001b[0m             )\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT2Model:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50257, 1536]) from checkpoint, the shape in current model is torch.Size([50264, 1536])."
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='Deniskin/gpt3_medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc7242f-e0c9-40d7-af66-674aa862f96f",
   "metadata": {},
   "source": [
    "The function below isn't used for now..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "190e334d-ed97-43c8-a37a-3bb4e9e50c5a",
   "metadata": {},
   "source": [
    "def generateNextQ():\n",
    "\n",
    "    # UNCOMMENT AFTER INTEGRATION WITH BACKEND\n",
    "\n",
    "    body_unicode = request.data.decode('utf-8')\n",
    "    body = json.loads(body_unicode)\n",
    "\n",
    "    print(\"Received body\", body)\n",
    "    text=body['qa_pair']\n",
    "\n",
    "    storage.append(text)\n",
    "\n",
    "    if len(starters) > 0: \n",
    "        print(\"SENDING STARTER\")\n",
    "        return {\"q\":starters.pop()}\n",
    "\n",
    "    else: \n",
    "\n",
    "        text = \" \".join(storage[-2:])\n",
    "        q = generator(text, num_return_sequences=3,max_length=50+len(text))\n",
    "\n",
    "        #all generated examples \n",
    "        allGenerations = \"\"\n",
    "        for i in range(3):\n",
    "            allGenerations = allGenerations +\" \"+ q[i]['generated_text'][len(text)-4:]\n",
    "        \n",
    "        #Separating all the sentences... \n",
    "        sentenceList = nltk.tokenize.sent_tokenize(allGenerations)\n",
    "\n",
    "        #Filter out questions \n",
    "        questionsList = []\n",
    "        for sentence in sentenceList :\n",
    "            if \"?\" in sentence:\n",
    "                questionsList.append(sentence.strip(\"\\n\").strip(\"\\\\\").strip('\"'))\n",
    "\n",
    "        #Bert evaluation\n",
    "        bert_filtered_qs = []\n",
    "        for sentence in questionsList:\n",
    "            encoding = tokenizer(\" \".join(storage[-3:]), sentence, return_tensors='pt')\n",
    "            outputs = model(**encoding)\n",
    "            logits = outputs.logits\n",
    "            bert_filtered_qs.append((logits[0,0].item(), sentence))\n",
    "\n",
    "\n",
    "        bert_filtered_qs.sort(key=lambda tup: tup[0])\n",
    "\n",
    "\n",
    "        print(bert_filtered_qs)\n",
    "\n",
    "        return {\"q\":bert_filtered_qs[-1][1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3711b-36c7-42b3-bc90-f282ced3541d",
   "metadata": {},
   "source": [
    "## Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ffa40950-6747-4027-95fa-123e528ba324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "SQL_URL = \"mysql+pymysql://root:password@localhost:3307/toia\"\n",
    "\n",
    "ENGINE = db.create_engine(SQL_URL)\n",
    "\n",
    "CONNECTION = ENGINE.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cbe5a835-d67a-41d8-a091-661a1fe152d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = text(\n",
    "    \"SELECT question FROM questions \\\n",
    "        WHERE trigger_suggester = 1 \\\n",
    "    ;\"\n",
    ")\n",
    "\n",
    "result_proxy = CONNECTION.execute(statement)\n",
    "\n",
    "result_set = result_proxy.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "07756cea-b79d-419c-a191-318581433f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is your name?', 'Where and when were you born?', 'What do you do for a living?']\n"
     ]
    }
   ],
   "source": [
    "starters = [qs[0] for qs in result_set]\n",
    "print(starters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f5827d6e-eaf2-4457-b23a-d497aa6357ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = text(\"\"\"\n",
    "    SELECT question_suggestions.id_question, questions.question, questions.suggested_type as type\n",
    "        FROM question_suggestions\n",
    "        INNER JOIN questions ON questions.id = question_suggestions.id_question\n",
    "        WHERE question_suggestions.toia_id = 1 AND question_suggestions.isPending = 1\n",
    "        ORDER BY question_suggestions.id_question DESC LIMIT 1;\n",
    "    \"\"\"\n",
    "                )\n",
    "\n",
    "# statement = text(\n",
    "#     \"SELECT question FROM questions \\\n",
    "#         WHERE trigger_suggester = 1 \\\n",
    "#     ;\"\n",
    "# )\n",
    "\n",
    "result_proxy = CONNECTION.execute(statement)\n",
    "\n",
    "result_set = result_proxy.fetchall()\n",
    "\n",
    "# storage = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "300f9289-1203-433c-a71c-ce1113a3ea37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "65dc5d89-fdbc-4f99-ae69-33eafd3e08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_trial_generateNextQ(text):\n",
    "# text = \"What's your name? My name is Alberto.\"\n",
    "\n",
    "    storage.append(text)\n",
    "\n",
    "    if len(starters) > 0: \n",
    "        print(\"SENDING STARTER\")\n",
    "        return {\"q\":starters.pop()}\n",
    "\n",
    "    else: \n",
    "\n",
    "        text = \" \".join(storage[-2:])\n",
    "        q = generator(text, num_return_sequences=3,max_length=50+len(text))\n",
    "\n",
    "        #all generated examples \n",
    "        allGenerations = \"\"\n",
    "        for i in range(3):\n",
    "            allGenerations = allGenerations +\" \"+ q[i]['generated_text'][len(text)-4:]\n",
    "\n",
    "        #Separating all the sentences... \n",
    "        sentenceList = nltk.tokenize.sent_tokenize(allGenerations)\n",
    "\n",
    "        #Filter out questions \n",
    "        questionsList = []\n",
    "        for sentence in sentenceList :\n",
    "            if \"?\" in sentence:\n",
    "                questionsList.append(sentence.strip(\"\\n\").strip(\"\\\\\").strip('\"'))\n",
    "\n",
    "        #Bert evaluation\n",
    "        bert_filtered_qs = []\n",
    "        for sentence in questionsList:\n",
    "            encoding = tokenizer(\" \".join(storage[-3:]), sentence, return_tensors='pt')\n",
    "            outputs = model(**encoding)\n",
    "            logits = outputs.logits\n",
    "            bert_filtered_qs.append((logits[0,0].item(), sentence))\n",
    "\n",
    "\n",
    "        bert_filtered_qs.sort(key=lambda tup: tup[0], reverse=True)\n",
    "\n",
    "        # print(bert_filtered_qs[:5])\n",
    "        # print({\"q\":questionsList})\n",
    "\n",
    "        no_suggestions = min(len(bert_filtered_qs) - 1, 5)\n",
    "        \n",
    "        suggestions = [bert_filtered_qs[i][1] for i in range(no_suggestions) if bert_filtered_qs[i][1] != bert_filtered_qs[i + 1][1]]\n",
    "\n",
    "        # return {\"q\":bert_filtered_qs[-1][1]}\n",
    "        return suggestions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f40aa58f-1975-474a-b4aa-e998a1b714ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Now if the police do a search for this person, will you say something to them?']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_trial_generateNextQ(\"Are them under the age of 18? Yes they are.\")\n",
    "\n",
    "# print(\"\\n#############\\n\", \n",
    "#       starters, \n",
    "#       \"\\n#############\\n\", \n",
    "#       storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "577f35f1-273a-433b-aae9-d84810df2754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.TextGenerationPipeline at 0x1540d22b0>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3817097-53ed-46ae-8d3e-db1863ce6ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

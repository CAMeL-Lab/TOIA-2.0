{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deeeb555-bc77-4457-bb8f-d1403a06f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1353aae9-ff9d-4967-a1f1-7512dd821948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db\n",
    "from sqlalchemy.sql import text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import re\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "NLP = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ba4c61-c95d-4488-8a07-9aff189551b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avatar_id = '1'\n",
    "stream_id = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12b33cc-d96d-496e-bb70-c04dccfc4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_url = \"mysql+mysqlconnector://root:3bR}M5Jz%d$7@localhost/toia\"\n",
    "\n",
    "engine = db.create_engine(sql_url)\n",
    "connection = engine.connect()\n",
    "metadata = db.MetaData()\n",
    "# videos = db.Table('video', metadata, autoload=True, autoload_with=engine)\n",
    "\n",
    "# avatar_kb = db.select([videos]).where(\n",
    "#     videos.columns.toia_id == avatar_id,\n",
    "#     videos.columns.private == 0,\n",
    "#     videos.columns.type.notin_([\"filler\", \"exit\"])\n",
    "# )\n",
    "\n",
    "statement = text(f\"\"\"SELECT stream_has_video.stream_id_stream, video.* \n",
    "    FROM video \n",
    "    INNER JOIN stream_has_video \n",
    "    ON video.id_video = stream_has_video.video_id_video \n",
    "    WHERE stream_id_stream = {stream_id}\n",
    "    AND toia_id = {avatar_id}\n",
    "    AND private = 0\n",
    "    AND type NOT IN ('filler', 'exit');\"\"\")\n",
    "\n",
    "# ResultProxy = connection.execute(avatar_kb)\n",
    "ResultProxy = connection.execute(statement)\n",
    "ResultSet = ResultProxy.fetchall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45882754-fedc-4c6c-90fe-f281fba54080",
   "metadata": {},
   "source": [
    "SQL equivalent to:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43cd31c5-5364-44de-b7c2-ccd7c8bcaf47",
   "metadata": {},
   "source": [
    "SELECT stream_has_video.stream_id_stream, video.* \n",
    "    FROM video \n",
    "    INNER JOIN stream_has_video \n",
    "    ON video.id_video = stream_has_video.video_id_video \n",
    "    WHERE stream_id_stream = <stream_id>;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88f7e56-4734-4472-8401-f33279ca9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avatar = pd.DataFrame(ResultSet, \n",
    "             columns=[\n",
    "                 'stream_id_stream',\n",
    "                 'id_video', \n",
    "                 'type', \n",
    "                 'toia_id', \n",
    "                 'idx', \n",
    "                 'private', \n",
    "                 'question', \n",
    "                 'answer', \n",
    "                 'language', \n",
    "                 'likes', \n",
    "                 'views',\n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9f0827-9536-4eed-b240-2c4461566bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stream_id_stream</th>\n",
       "      <th>id_video</th>\n",
       "      <th>type</th>\n",
       "      <th>toia_id</th>\n",
       "      <th>idx</th>\n",
       "      <th>private</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>language</th>\n",
       "      <th>likes</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alberto_1_3_4958c7ed.mp4</td>\n",
       "      <td>greeting</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Record a greeting!</td>\n",
       "      <td>hello there. How's it going?</td>\n",
       "      <td>EN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Alberto_1_4_2dacf448.mp4</td>\n",
       "      <td>answer</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Where are you from?</td>\n",
       "      <td>I come from Italy</td>\n",
       "      <td>EN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stream_id_stream                  id_video      type  toia_id  idx  \\\n",
       "0                 1  Alberto_1_3_4958c7ed.mp4  greeting        1    3   \n",
       "1                 1  Alberto_1_4_2dacf448.mp4    answer        1    4   \n",
       "\n",
       "   private             question                        answer language  likes  \\\n",
       "0        0   Record a greeting!  hello there. How's it going?       EN      0   \n",
       "1        0  Where are you from?             I come from Italy       EN      0   \n",
       "\n",
       "   views  \n",
       "0      0  \n",
       "1      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avatar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f1d383b-f0c4-4fcf-8758-3a70d083949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- next doc ---\n",
      "Do you like Biden?\n",
      "Do AUX VBP\n",
      "you PRON PRP\n",
      "like ADP IN\n",
      "Biden PROPN NNP\n",
      "? PUNCT .\n",
      "Biden 12 17 PERSON\n",
      "--- next doc ---\n",
      "Do you smoke?\n",
      "Do AUX VBP\n",
      "you PRON PRP\n",
      "smoke VERB VB\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "Do you have any pets?\n",
      "Do AUX VBP\n",
      "you PRON PRP\n",
      "have VERB VB\n",
      "any DET DT\n",
      "pets NOUN NNS\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "Record a greeting!\n",
      "Record VERB VB\n",
      "a DET DT\n",
      "greeting NOUN NN\n",
      "! PUNCT .\n",
      "--- next doc ---\n",
      "Where are you from?\n",
      "Where ADV WRB\n",
      "are AUX VBP\n",
      "you PRON PRP\n",
      "from ADP IN\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "Do you have any siblings?\n",
      "Do AUX VBP\n",
      "you PRON PRP\n",
      "have VERB VB\n",
      "any DET DT\n",
      "siblings NOUN NNS\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "Do you smoke?\n",
      "Do AUX VBP\n",
      "you PRON PRP\n",
      "smoke VERB VB\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "What's your name?\n",
      "What PRON WP\n",
      "'s AUX VBZ\n",
      "your PRON PRP$\n",
      "name NOUN NN\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "Do you have any hobbies?\n",
      "Do AUX VBP\n",
      "you PRON PRP\n",
      "have VERB VB\n",
      "any DET DT\n",
      "hobbies NOUN NNS\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "What is your favorite movie?\n",
      "What PRON WP\n",
      "is AUX VBZ\n",
      "your PRON PRP$\n",
      "favorite ADJ JJ\n",
      "movie NOUN NN\n",
      "? PUNCT .\n"
     ]
    }
   ],
   "source": [
    "docs = NLP.pipe(df_avatar['question'].values)\n",
    "for doc in docs:\n",
    "    print(\"--- next doc ---\")\n",
    "    print(doc)\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.tag_)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba228ea6-23a1-4fe1-b0d3-a3e837b1ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = NLP(\"hey, hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9074bb1a-720d-41cc-9239-76ea15022d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['INTJ', 'UH'] in [[token.pos_, token.tag_] for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f0f3649-fba5-4515-abdc-7e84c3e784ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',', '.', 'UH'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([token.tag_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c4f751d-c475-4465-b6c0-7637647fe331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_greetings = df_avatar[df_avatar['type'] == \"greeting\"]\n",
    "if df_greetings.shape[0] > 0:\n",
    "    df_greetings.sample(n=1)['answer'].values[0]\n",
    "else:\n",
    "    print(\"204 No Content: you haven't recorded greetings\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bebbb785-0e26-4938-a4e6-5bbe15691122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    base = NLP(process_text(text1))\n",
    "    compare = NLP(process_text(text2))\n",
    "    return base.similarity(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4b14530-483d-4437-a5ad-36ec9df0fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    doc = NLP(text.lower())\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if token.text in NLP.Defaults.stop_words:\n",
    "            continue\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.lemma_ == '-PRON-':\n",
    "            continue\n",
    "        result.append(token.lemma_)\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bb204c7-cd82-4bc9-bec4-d0e3d87f0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = SnowballStemmer('english')\n",
    "\n",
    "def preprocess(text):\n",
    "            # Stem and remove stopwords\n",
    "            text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "            text = text.lower()\n",
    "            text = text.split()\n",
    "            text = [ps.stem(word) for word in text]  # if not word in set(stopwords.words('english'))]\n",
    "            return ' '.join(text)\n",
    "\n",
    "\n",
    "def toia_answer(query, dataset, k=1):\n",
    "    doc = NLP(query)\n",
    "    # if Greeting, greet\n",
    "    if ['INTJ', 'UH'] in [[token.pos_, token.tag_] for token in doc]:    \n",
    "        if df_greetings.shape[0] > 0:\n",
    "            answers = dataset[dataset['type'] == \"greeting\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0]\n",
    "        else:\n",
    "            df_noanswers = dataset[dataset['type'] == \"no-answer\"]\n",
    "            if df_noanswers.shape[0] > 0:\n",
    "                answers = df_noanswers.sample(n=1)\n",
    "                return answers['answer'].values[0], answers['id_video'].values[0], \"Record some reetings\"\n",
    "            else:\n",
    "                return \"You haven't recorded greetings nor no-answers\", \"204\", \"No Content\"\n",
    "\n",
    "    querycorpus = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        userquestion = preprocess(dataset['question'][i])\n",
    "        querycorpus.append(userquestion)\n",
    "\n",
    "    # Creating the Bag of Words model with TFIDF and calc cosine_similarity\n",
    "    vectorizer = CountVectorizer(decode_error=\"replace\")\n",
    "    # this is needed to get the attribute vocabulary_\n",
    "    vec_train = vectorizer.fit_transform(querycorpus)\n",
    "    training_vocabulary = vectorizer.vocabulary_\n",
    "    transformer = TfidfTransformer()\n",
    "    trainingvoc_vectorizer = CountVectorizer(\n",
    "        decode_error=\"replace\", vocabulary=training_vocabulary)\n",
    "    tfidf_querycorpus = TfidfVectorizer().fit_transform(querycorpus)\n",
    "\n",
    "    tfidf_userquestion = transformer.fit_transform(\n",
    "        trainingvoc_vectorizer.fit_transform(\n",
    "            numpy.array([\n",
    "                preprocess(query)\n",
    "            ])))\n",
    "    cosine_similarities = cosine_similarity(tfidf_userquestion, tfidf_querycorpus)\n",
    "    related_docs_indices = (-cosine_similarities[0]).argsort()\n",
    "    sorted_freq = cosine_similarities[0][related_docs_indices]\n",
    "\n",
    "    # note for this distance the problem we had befor with inf, we have now with 0. Again we decide\n",
    "    # to make the prediction a bit random. This could be adjusted to remove any 0 distance and\n",
    "    # pick the only ones left if any, and if none predict 1.\n",
    "\n",
    "    if sum(sorted_freq) == 0:\n",
    "        df_noanswers = dataset[dataset['type'] == \"no-answer\"]\n",
    "        if df_noanswers.shape[0] > 0:\n",
    "            answers = df_noanswers.sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0], \"tfidf all sim 0\"\n",
    "        else:\n",
    "            return \"You haven't recorded no-answers\", \"204\", \"No Content\"\n",
    "    elif sorted_freq[0] > 0.7:  #(the top sorted freq is the max)\n",
    "        if sorted_freq[k-1] != sorted_freq[k] or sorted_freq[k-1] == sorted_freq[k] == 0:\n",
    "            selected = related_docs_indices[:k]\n",
    "            return dataset.iloc[selected[0]]['answer'], dataset.iloc[selected[0]]['id_video'], f\"tfidf sim: {sorted_freq[:k]}\"\n",
    "        else:\n",
    "            indeces = numpy.where(numpy.roll(sorted_freq, 1) != sorted_freq)\n",
    "            selected = related_docs_indices[:indeces[0][indeces[0] >= k][0]]\n",
    "            return dataset.iloc[selected[0]]['answer'], dataset.iloc[selected[0]]['id_video'], f\"tfidf sim: {sorted_freq[:k]}\"\n",
    "\n",
    "    else:\n",
    "        docs = NLP.pipe(dataset['question'].values)\n",
    "        cosine_similarities = [calculate_similarity(query, doc.text) for doc in docs]\n",
    "        if max(cosine_similarities) > 0.5:\n",
    "            related_docs_indices = np.argsort(cosine_similarities)[::-1]\n",
    "            selected = related_docs_indices[:k][0]\n",
    "            return dataset.iloc[selected]['answer'], dataset.iloc[selected]['id_video'], f\"spaCy sim: {cosine_similarities[selected]}\"\n",
    "        else:\n",
    "            df_noanswers = dataset[dataset['type'] == \"no-answer\"]\n",
    "            if df_noanswers.shape[0] > 0:\n",
    "                answers = df_noanswers.sample(n=1)\n",
    "                return answers['answer'].values[0], answers['id_video'].values[0], \"spaCy sim below thr\"\n",
    "            else:\n",
    "                return \"You haven't recorded no-answers\", \"204\", \"No Content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cd47cf2-2ea4-42cb-9bc3-9d9127053b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"sorry I don't think I have recorded an answer to this question\",\n",
       " 'Alberto_1_14_74b81db9.mp4',\n",
       " 'tfidf sim: [0.73538024]')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toia_answer(\"Do you like\", df_avatar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1630e-89fd-4eb7-8d9a-a88c0ad1d7ff",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dee8b9-ebd5-4230-9869-182b193e9f20",
   "metadata": {},
   "source": [
    "## Experiment with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "820f850f-d2cc-42b7-ae3d-2d185fdbfcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yh/m455kn5x5pbgwq0gg3dt4xtc0000gn/T/ipykernel_36543/2174577701.py:4: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  return base.similarity(compare)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2515495048348726,\n",
       " 0.23574273430171253,\n",
       " 0.1766564625961641,\n",
       " 0.1750639131172708,\n",
       " 0.0,\n",
       " 0.07056772251277416,\n",
       " 0.23574273430171253,\n",
       " 0.0,\n",
       " 0.18860034048541044,\n",
       " 0.2306338946800242]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = NLP.pipe(df_avatar['question'].values)\n",
    "similar_list = [calculate_similarity(\"Yo!\", doc.text) for doc in docs]\n",
    "similar_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddb2ba8e-45a6-46ea-8b21-f71d0569cefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Do you like Biden?', 0.2515495048348726),\n",
       " ('Do you smoke?', 0.23574273430171253),\n",
       " ('Do you smoke?', 0.23574273430171253),\n",
       " ('What is your favorite movie?', 0.2306338946800242),\n",
       " ('Do you have any hobbies?', 0.18860034048541044),\n",
       " ('Do you have any pets?', 0.1766564625961641),\n",
       " ('Record a greeting!', 0.1750639131172708),\n",
       " ('Do you have any siblings?', 0.07056772251277416),\n",
       " (\"What's your name?\", 0.0),\n",
       " ('Where are you from?', 0.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(df_avatar.loc[j, 'question'], i) for\n",
    " i, j in zip(\n",
    "     np.sort(similar_list)[::-1], \n",
    "     np.argsort(similar_list)[::-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485590a0-eeff-4fd8-a502-8c73f5ebd3a2",
   "metadata": {},
   "source": [
    "### Test new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6b53024-656e-4af6-aa56-1f8806001ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"what's your name\"\n",
    "# dataset = df_avatar.copy()\n",
    "# k = 1\n",
    "\n",
    "def toia_answer_new(query, dataset, k=1):\n",
    "    doc = NLP(query)\n",
    "    # if Greeting, greet\n",
    "    if ['INTJ', 'UH'] in [[token.pos_, token.tag_] for token in doc]:\n",
    "        if dataset[dataset['type'] == \"greeting\"].shape[0] > 0:\n",
    "            answers = dataset[dataset['type'] == \"greeting\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0]\n",
    "        else:\n",
    "            answers = dataset[dataset['type'] == \"no-answer\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0], \"No greetings recorded\"\n",
    "\n",
    "    querycorpus = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        userquestion = preprocess(dataset['question'][i])\n",
    "        querycorpus.append(userquestion)\n",
    "\n",
    "    # Creating the Bag of Words model with TFIDF and calc cosine_similarity\n",
    "    vectorizer = CountVectorizer(decode_error=\"replace\")\n",
    "    # this is needed to get the attribute vocabulary_\n",
    "    vec_train = vectorizer.fit_transform(querycorpus)\n",
    "    training_vocabulary = vectorizer.vocabulary_\n",
    "    transformer = TfidfTransformer()\n",
    "    trainingvoc_vectorizer = CountVectorizer(\n",
    "        decode_error=\"replace\", vocabulary=training_vocabulary)\n",
    "    tfidf_querycorpus = TfidfVectorizer().fit_transform(querycorpus)\n",
    "\n",
    "    tfidf_userquestion = transformer.fit_transform(\n",
    "        trainingvoc_vectorizer.fit_transform(\n",
    "            numpy.array([\n",
    "                preprocess(query)\n",
    "            ])))\n",
    "    cosine_similarities = cosine_similarity(tfidf_userquestion, tfidf_querycorpus)\n",
    "    related_docs_indices = (-cosine_similarities[0]).argsort()\n",
    "    sorted_freq = cosine_similarities[0][related_docs_indices]\n",
    "\n",
    "    # note for this distance the problem we had befor with inf, we have now with 0. Again we decide\n",
    "    # to make the prediction a bit random. This could be adjusted to remove any 0 distance and\n",
    "    # pick the only ones left if any, and if none predict 1.\n",
    "\n",
    "    if sum(sorted_freq) == 0:\n",
    "        answers = dataset[dataset['type'] == \"no-answer\"].sample(n=1)\n",
    "        return answers['answer'].values[0], answers['id_video'].values[0], \"tfidf sim all 0\"\n",
    "\n",
    "    elif sorted_freq[0] > 0.7:  #(the top sorted freq is the max)\n",
    "        if sorted_freq[k-1] != sorted_freq[k] or sorted_freq[k-1] == sorted_freq[k] == 0:\n",
    "            selected = related_docs_indices[:k]\n",
    "            return dataset.iloc[selected[0]]['answer'], dataset.iloc[selected[0]]['id_video'], f\"tfidf sim: {sorted_freq[:k]}\"\n",
    "        else:\n",
    "            indeces = numpy.where(numpy.roll(sorted_freq, 1) != sorted_freq)\n",
    "            selected = related_docs_indices[:indeces[0][indeces[0] >= k][0]]\n",
    "            return dataset.iloc[selected[0]]['answer'], dataset.iloc[selected[0]]['id_video'], f\"tfidf sim: {sorted_freq[:k]}\"\n",
    "\n",
    "    else:\n",
    "        docs = NLP.pipe(dataset['question'].values)\n",
    "        cosine_similarities = [calculate_similarity(query, doc.text) for doc in docs]\n",
    "        if max(cosine_similarities) > 0.5:\n",
    "            related_docs_indices = np.argsort(cosine_similarities)[::-1]\n",
    "            selected = related_docs_indices[:k][0]\n",
    "            return dataset.iloc[selected]['answer'], dataset.iloc[selected]['id_video'], f\"spaCy sim: {cosine_similarities[selected]}\"\n",
    "        else:\n",
    "            answers = dataset[dataset['type'] == \"no-answer\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0], f\"spaCy sim: {max(cosine_similarities)}\"\n",
    "\n",
    "            \n",
    "# For testing function, convert all 'return' to 'return_a ='\n",
    "# return_a  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b0ae6c4-4193-46a8-bfeb-dfd1dc856f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_toia(dataset):\n",
    "    print(\"TOIA starts\")\n",
    "\n",
    "    while True:\n",
    "        query = input('> ')\n",
    "        if query == \"stop\":\n",
    "            break\n",
    "\n",
    "        output = toia_answer_new(query, dataset)\n",
    "        if output is None:\n",
    "            break\n",
    "\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0f76aba-77c3-40dc-a8a5-c998b901693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOIA starts\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  this is a test question\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Indeed it is a test', 'Alberto_1_17_552fef0d.mp4', 'tfidf sim: [0.99787743]')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  hullo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"hello what's up?\", 'Alberto_1_2_07d0b328.mp4')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  nice\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yh/m455kn5x5pbgwq0gg3dt4xtc0000gn/T/ipykernel_36543/1924364141.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_toia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_avatar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/yh/m455kn5x5pbgwq0gg3dt4xtc0000gn/T/ipykernel_36543/90087033.py\u001b[0m in \u001b[0;36mrun_toia\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoia_answer_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/yh/m455kn5x5pbgwq0gg3dt4xtc0000gn/T/ipykernel_36543/1174556008.py\u001b[0m in \u001b[0;36mtoia_answer_new\u001b[0;34m(query, dataset, k)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_freq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"no-answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tfidf sim all 0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/dmserv-hxg3N1X-/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5363\u001b[0m             )\n\u001b[1;32m   5364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5365\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5366\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "run_toia(df_avatar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db9272-4b20-453e-8bff-2aff711905fd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bca89-98ca-4462-bcc9-7e15de0013f9",
   "metadata": {},
   "source": [
    "## WIP: Can see if an ELIZA-like algorithm can help\n",
    "\n",
    "https://github.com/wadetb/eliza\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211ad7e-149c-462d-a101-ed309b92e538",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

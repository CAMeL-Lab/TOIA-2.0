{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeeb555-bc77-4457-bb8f-d1403a06f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1353aae9-ff9d-4967-a1f1-7512dd821948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import re\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "NLP = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "18ba4c61-c95d-4488-8a07-9aff189551b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avatar_id = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e12b33cc-d96d-496e-bb70-c04dccfc4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_url = \"mysql+mysqlconnector://root@localhost/toia\"\n",
    "\n",
    "engine = db.create_engine(sql_url)\n",
    "connection = engine.connect()\n",
    "metadata = db.MetaData()\n",
    "videos = db.Table('video', metadata, autoload=True, autoload_with=engine)\n",
    "\n",
    "avatar_kb = db.select([videos]).where(\n",
    "    videos.columns.toia_id == avatar_id,\n",
    "    videos.columns.private == 0,\n",
    "    videos.columns.type.notin_([\"filler\", \"exit\"])\n",
    ")\n",
    "\n",
    "ResultProxy = connection.execute(avatar_kb)\n",
    "ResultSet = ResultProxy.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e88f7e56-4734-4472-8401-f33279ca9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avatar = pd.DataFrame(ResultSet, \n",
    "             columns=[\n",
    "                 'id_video', \n",
    "                 'type', \n",
    "                 'toia_id', \n",
    "                 'idx', \n",
    "                 'private', \n",
    "                 'question', \n",
    "                 'answer', \n",
    "                 'language', \n",
    "                 'likes', \n",
    "                 'views',\n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f9f0827-9536-4eed-b240-2c4461566bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_video</th>\n",
       "      <th>type</th>\n",
       "      <th>toia_id</th>\n",
       "      <th>idx</th>\n",
       "      <th>private</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>language</th>\n",
       "      <th>likes</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b2</td>\n",
       "      <td>answer</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>What is your favorite sport?</td>\n",
       "      <td>I love soccer!</td>\n",
       "      <td>en-US</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a7c</td>\n",
       "      <td>answer</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>What do you study?</td>\n",
       "      <td>I study Computer Science.</td>\n",
       "      <td>en-US</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ef1</td>\n",
       "      <td>answer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>How are you?</td>\n",
       "      <td>I am fine thanks!</td>\n",
       "      <td>en-US</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2a</td>\n",
       "      <td>answer</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>What do you do?</td>\n",
       "      <td>I am doing a Ph.D.</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2b</td>\n",
       "      <td>answer</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>How old are you?</td>\n",
       "      <td>I'm 35. Age last birthday</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>r2c</td>\n",
       "      <td>answer</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Tell me something interesting?</td>\n",
       "      <td>I wrote a book about the ethics of artificial ...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>r2e</td>\n",
       "      <td>greeting</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>hey</td>\n",
       "      <td>how's it going?</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>r2f</td>\n",
       "      <td>greeting</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Hello</td>\n",
       "      <td>Hi!</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>r2h</td>\n",
       "      <td>no-answer</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Sorry, I didn't get that</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>r2m</td>\n",
       "      <td>greeting</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Howdy</td>\n",
       "      <td>hey, I'm all right</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>r2t</td>\n",
       "      <td>y/n-answer</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Do you like sushi?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>r2y</td>\n",
       "      <td>no-answer</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>I don't have an answer for that</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>r4t</td>\n",
       "      <td>y/n-answer</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Do you like swimming?</td>\n",
       "      <td>No</td>\n",
       "      <td>en-US</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_video        type  toia_id  idx  private  \\\n",
       "0       0b2      answer        1    2        0   \n",
       "1       a7c      answer        1    3        0   \n",
       "2       ef1      answer        1    1        0   \n",
       "3       r2a      answer        1    7        0   \n",
       "4       r2b      answer        1    8        0   \n",
       "5       r2c      answer        1    9        0   \n",
       "6       r2e    greeting        1   16        0   \n",
       "7       r2f    greeting        1   11        0   \n",
       "8       r2h   no-answer        1   12        0   \n",
       "9       r2m    greeting        1    6        0   \n",
       "10      r2t  y/n-answer        1   13        0   \n",
       "11      r2y   no-answer        1   15        0   \n",
       "12      r4t  y/n-answer        1   14        0   \n",
       "\n",
       "                          question  \\\n",
       "0     What is your favorite sport?   \n",
       "1               What do you study?   \n",
       "2                     How are you?   \n",
       "3                  What do you do?   \n",
       "4                 How old are you?   \n",
       "5   Tell me something interesting?   \n",
       "6                              hey   \n",
       "7                            Hello   \n",
       "8                                    \n",
       "9                            Howdy   \n",
       "10              Do you like sushi?   \n",
       "11                                   \n",
       "12           Do you like swimming?   \n",
       "\n",
       "                                               answer language  likes  views  \n",
       "0                                      I love soccer!    en-US      2      5  \n",
       "1                           I study Computer Science.    en-US     10     34  \n",
       "2                                   I am fine thanks!    en-US      5     14  \n",
       "3                                  I am doing a Ph.D.    en-US      4     20  \n",
       "4                           I'm 35. Age last birthday    en-US      4     20  \n",
       "5   I wrote a book about the ethics of artificial ...    en-US      4     20  \n",
       "6                                     how's it going?    en-US      4     20  \n",
       "7                                                 Hi!    en-US      4     20  \n",
       "8                            Sorry, I didn't get that    en-US      4     20  \n",
       "9                                  hey, I'm all right    en-US      4     20  \n",
       "10                                                Yes    en-US      4     20  \n",
       "11                    I don't have an answer for that    en-US      4     20  \n",
       "12                                                 No    en-US      4     20  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avatar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0f1d383b-f0c4-4fcf-8758-3a70d083949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- next doc ---\n",
      "What is your favorite sport?\n",
      "What PRON WP\n",
      "is AUX VBZ\n",
      "your PRON PRP$\n",
      "favorite ADJ JJ\n",
      "sport NOUN NN\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "What do you study?\n",
      "What PRON WP\n",
      "do AUX VBP\n",
      "you PRON PRP\n",
      "study VERB VB\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "How are you?\n",
      "How ADV WRB\n",
      "are AUX VBP\n",
      "you PRON PRP\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "What do you do?\n",
      "What PRON WP\n",
      "do AUX VBP\n",
      "you PRON PRP\n",
      "do VERB VB\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "How old are you?\n",
      "How ADV WRB\n",
      "old ADJ JJ\n",
      "are AUX VBP\n",
      "you PRON PRP\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "Tell me something interesting?\n",
      "Tell VERB VB\n",
      "me PRON PRP\n",
      "something PRON NN\n",
      "interesting ADJ JJ\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "hey\n",
      "hey INTJ UH\n",
      "--- next doc ---\n",
      "Hello\n",
      "Hello INTJ UH\n",
      "--- next doc ---\n",
      "\n",
      "--- next doc ---\n",
      "Howdy\n",
      "Howdy INTJ UH\n",
      "--- next doc ---\n",
      "Do you like sushi?\n",
      "Do AUX VBP\n",
      "you PRON PRP\n",
      "like VERB VB\n",
      "sushi NOUN NN\n",
      "? PUNCT .\n",
      "--- next doc ---\n",
      "\n",
      "--- next doc ---\n",
      "Do you like swimming?\n",
      "Do AUX VBP\n",
      "you PRON PRP\n",
      "like VERB VB\n",
      "swimming VERB VBG\n",
      "? PUNCT .\n"
     ]
    }
   ],
   "source": [
    "docs = NLP.pipe(df_avatar['question'].values)\n",
    "for doc in docs:\n",
    "    print(\"--- next doc ---\")\n",
    "    print(doc)\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.tag_)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ba228ea6-23a1-4fe1-b0d3-a3e837b1ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = NLP(\"hey, hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9074bb1a-720d-41cc-9239-76ea15022d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['INTJ', 'UH'] in [[token.pos_, token.tag_] for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8f0f3649-fba5-4515-abdc-7e84c3e784ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',', '.', 'UH'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([token.tag_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3c4f751d-c475-4465-b6c0-7637647fe331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hey, I'm all right\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avatar[df_avatar['type'] == \"greeting\"].sample(n=1)['answer'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0bb204c7-cd82-4bc9-bec4-d0e3d87f0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = SnowballStemmer('english')\n",
    "\n",
    "def preprocess(text):\n",
    "            # Stem and remove stopwords\n",
    "            text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "            text = text.lower()\n",
    "            text = text.split()\n",
    "            text = [ps.stem(word) for word in text]  # if not word in set(stopwords.words('english'))]\n",
    "            return ' '.join(text)\n",
    "\n",
    "\n",
    "def toia_answer(query, dataset, k=1):\n",
    "    doc = NLP(query)\n",
    "    # if Greeting, greet\n",
    "    if ['INTJ', 'UH'] in [[token.pos_, token.tag_] for token in doc]:\n",
    "        if dataset[dataset['type'] == \"greeting\"].shape[0] > 0:\n",
    "            answers = dataset[dataset['type'] == \"greeting\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0]\n",
    "        else:\n",
    "            answers = dataset[dataset['type'] == \"no-answer\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0], \"No greetings recorded\"\n",
    "\n",
    "    querycorpus = []\n",
    "    for i in range(0, len(df_avatar)):\n",
    "        userquestion = preprocess(df_avatar['question'][i])\n",
    "        querycorpus.append(userquestion)\n",
    "\n",
    "    # Creating the Bag of Words model with TFIDF and calc cosine_similarity\n",
    "    vectorizer = CountVectorizer(decode_error=\"replace\")\n",
    "    # this is needed to get the attribute vocabulary_\n",
    "    vec_train = vectorizer.fit_transform(querycorpus)\n",
    "    training_vocabulary = vectorizer.vocabulary_\n",
    "    transformer = TfidfTransformer()\n",
    "    trainingvoc_vectorizer = CountVectorizer(\n",
    "        decode_error=\"replace\", vocabulary=training_vocabulary)\n",
    "    tfidf_querycorpus = TfidfVectorizer().fit_transform(querycorpus)\n",
    "\n",
    "    tfidf_userquestion = transformer.fit_transform(\n",
    "        trainingvoc_vectorizer.fit_transform(\n",
    "            numpy.array([\n",
    "                preprocess(query)\n",
    "            ])))\n",
    "    cosine_similarities = cosine_similarity(tfidf_userquestion, tfidf_querycorpus)\n",
    "    related_docs_indices = (-cosine_similarities[0]).argsort()\n",
    "    sorted_freq = cosine_similarities[0][related_docs_indices]\n",
    "\n",
    "    # note for this distance the problem we had befor with inf, we have now with 0. Again we decide\n",
    "    # to make the prediction a bit random. This could be adjusted to remove any 0 distance and\n",
    "    # pick the only ones left if any, and if none predict 1.\n",
    "\n",
    "    if sum(sorted_freq) == 0:\n",
    "        answers = dataset[dataset['type'] == \"no-answer\"].sample(n=1)\n",
    "        return answers['answer'].values[0], answers['id_video'].values[0], \"tfidf sim all 0\"\n",
    "\n",
    "    elif sorted_freq[0] > 0.7:  #(the top sorted freq is the max)\n",
    "        if sorted_freq[k-1] != sorted_freq[k] or sorted_freq[k-1] == sorted_freq[k] == 0:\n",
    "            selected = related_docs_indices[:k]\n",
    "            return dataset.iloc[selected[0]]['answer'], dataset.iloc[selected[0]]['id_video'], f\"tfidf sim: {sorted_freq[:k]}\"\n",
    "        else:\n",
    "            indeces = numpy.where(numpy.roll(sorted_freq, 1) != sorted_freq)\n",
    "            selected = related_docs_indices[:indeces[0][indeces[0] >= k][0]]\n",
    "            return dataset.iloc[selected[0]]['answer'], dataset.iloc[selected[0]]['id_video'], f\"tfidf sim: {sorted_freq[:k]}\"\n",
    "\n",
    "    else:\n",
    "        docs = NLP.pipe(dataset['question'].values)\n",
    "        cosine_similarities = [calculate_similarity(query, doc.text) for doc in docs]\n",
    "        if max(cosine_similarities) > 0.5:\n",
    "            related_docs_indices = np.argsort(cosine_similarities)[::-1]\n",
    "            selected = related_docs_indices[:k][0]\n",
    "            return dataset.iloc[selected]['answer'], dataset.iloc[selected]['id_video'], f\"spaCy sim: {cosine_similarities[selected]}\"\n",
    "        else:\n",
    "            answers = dataset[dataset['type'] == \"no-answer\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0], f\"spaCy sim: {max(cosine_similarities)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4cd47cf2-2ea4-42cb-9bc3-9d9127053b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I don't have an answer for that\", 'r2y')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toia_answer(\"hola cachina\", df_avatar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1630e-89fd-4eb7-8d9a-a88c0ad1d7ff",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dee8b9-ebd5-4230-9869-182b193e9f20",
   "metadata": {},
   "source": [
    "## Experiment with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d4b14530-483d-4437-a5ad-36ec9df0fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    doc = NLP(text.lower())\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if token.text in NLP.Defaults.stop_words:\n",
    "            continue\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.lemma_ == '-PRON-':\n",
    "            continue\n",
    "        result.append(token.lemma_)\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bebbb785-0e26-4938-a4e6-5bbe15691122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    base = NLP(process_text(text1))\n",
    "    compare = NLP(process_text(text2))\n",
    "    return base.similarity(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "820f850f-d2cc-42b7-ae3d-2d185fdbfcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-147-5b3d2fc83f3d>:4: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  return base.similarity(compare)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1960290722844779,\n",
       " 0.07379277395621531,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3354120586746895,\n",
       " 0.29740283310352034,\n",
       " 0.5982720020715394,\n",
       " 0.4097530789445466,\n",
       " 0.0,\n",
       " 0.33224449180378135,\n",
       " 0.3386126303165402,\n",
       " 0.0,\n",
       " 0.30997427744569134]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = NLP.pipe(df_avatar['question'].values)\n",
    "similar_list = [calculate_similarity(\"Yo!\", doc.text) for doc in docs]\n",
    "similar_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ddb2ba8e-45a6-46ea-8b21-f71d0569cefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hey', 0.5982720020715394),\n",
       " ('Hello', 0.4097530789445466),\n",
       " ('Do you like sushi?', 0.3386126303165402),\n",
       " ('How old are you?', 0.3354120586746895),\n",
       " ('Howdy', 0.33224449180378135),\n",
       " ('Do you like swimming?', 0.30997427744569134),\n",
       " ('Tell me something interesting?', 0.29740283310352034),\n",
       " ('What is your favorite sport?', 0.1960290722844779),\n",
       " ('What do you study?', 0.07379277395621531),\n",
       " ('', 0.0),\n",
       " ('', 0.0),\n",
       " ('What do you do?', 0.0),\n",
       " ('How are you?', 0.0)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(df_avatar.loc[j, 'question'], i) for\n",
    " i, j in zip(\n",
    "     np.sort(similar_list)[::-1], \n",
    "     np.argsort(similar_list)[::-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485590a0-eeff-4fd8-a502-8c73f5ebd3a2",
   "metadata": {},
   "source": [
    "### Test new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ab1bbeda-19a1-4575-b23a-9d3a36e232ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['type'] == \"abal\"].shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d6b53024-656e-4af6-aa56-1f8806001ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"what's your name\"\n",
    "# dataset = df_avatar.copy()\n",
    "# k = 1\n",
    "\n",
    "def toia_answer_new(query, dataset, k=1):\n",
    "    doc = NLP(query)\n",
    "    # if Greeting, greet\n",
    "    if ['INTJ', 'UH'] in [[token.pos_, token.tag_] for token in doc]:\n",
    "        if dataset[dataset['type'] == \"greeting\"].shape[0] > 0:\n",
    "            answers = dataset[dataset['type'] == \"greeting\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0]\n",
    "        else:\n",
    "            answers = dataset[dataset['type'] == \"no-answer\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0], \"No greetings recorded\"\n",
    "\n",
    "    querycorpus = []\n",
    "    for i in range(0, len(df_avatar)):\n",
    "        userquestion = preprocess(df_avatar['question'][i])\n",
    "        querycorpus.append(userquestion)\n",
    "\n",
    "    # Creating the Bag of Words model with TFIDF and calc cosine_similarity\n",
    "    vectorizer = CountVectorizer(decode_error=\"replace\")\n",
    "    # this is needed to get the attribute vocabulary_\n",
    "    vec_train = vectorizer.fit_transform(querycorpus)\n",
    "    training_vocabulary = vectorizer.vocabulary_\n",
    "    transformer = TfidfTransformer()\n",
    "    trainingvoc_vectorizer = CountVectorizer(\n",
    "        decode_error=\"replace\", vocabulary=training_vocabulary)\n",
    "    tfidf_querycorpus = TfidfVectorizer().fit_transform(querycorpus)\n",
    "\n",
    "    tfidf_userquestion = transformer.fit_transform(\n",
    "        trainingvoc_vectorizer.fit_transform(\n",
    "            numpy.array([\n",
    "                preprocess(query)\n",
    "            ])))\n",
    "    cosine_similarities = cosine_similarity(tfidf_userquestion, tfidf_querycorpus)\n",
    "    related_docs_indices = (-cosine_similarities[0]).argsort()\n",
    "    sorted_freq = cosine_similarities[0][related_docs_indices]\n",
    "\n",
    "    # note for this distance the problem we had befor with inf, we have now with 0. Again we decide\n",
    "    # to make the prediction a bit random. This could be adjusted to remove any 0 distance and\n",
    "    # pick the only ones left if any, and if none predict 1.\n",
    "\n",
    "    if sum(sorted_freq) == 0:\n",
    "        answers = dataset[dataset['type'] == \"no-answer\"].sample(n=1)\n",
    "        return answers['answer'].values[0], answers['id_video'].values[0], \"tfidf sim all 0\"\n",
    "\n",
    "    elif sorted_freq[0] > 0.7:  #(the top sorted freq is the max)\n",
    "        if sorted_freq[k-1] != sorted_freq[k] or sorted_freq[k-1] == sorted_freq[k] == 0:\n",
    "            selected = related_docs_indices[:k]\n",
    "            return dataset.iloc[selected[0]]['answer'], dataset.iloc[selected[0]]['id_video'], f\"tfidf sim: {sorted_freq[:k]}\"\n",
    "        else:\n",
    "            indeces = numpy.where(numpy.roll(sorted_freq, 1) != sorted_freq)\n",
    "            selected = related_docs_indices[:indeces[0][indeces[0] >= k][0]]\n",
    "            return dataset.iloc[selected[0]]['answer'], dataset.iloc[selected[0]]['id_video'], f\"tfidf sim: {sorted_freq[:k]}\"\n",
    "\n",
    "    else:\n",
    "        docs = NLP.pipe(dataset['question'].values)\n",
    "        cosine_similarities = [calculate_similarity(query, doc.text) for doc in docs]\n",
    "        if max(cosine_similarities) > 0.5:\n",
    "            related_docs_indices = np.argsort(cosine_similarities)[::-1]\n",
    "            selected = related_docs_indices[:k][0]\n",
    "            return dataset.iloc[selected]['answer'], dataset.iloc[selected]['id_video'], f\"spaCy sim: {cosine_similarities[selected]}\"\n",
    "        else:\n",
    "            answers = dataset[dataset['type'] == \"no-answer\"].sample(n=1)\n",
    "            return answers['answer'].values[0], answers['id_video'].values[0], f\"spaCy sim: {max(cosine_similarities)}\"\n",
    "\n",
    "            \n",
    "# For testing function, convert all 'return' to 'return_a ='\n",
    "# return_a  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9b0ae6c4-4193-46a8-bfeb-dfd1dc856f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_toia(dataset):\n",
    "    print(\"TOIA starts\")\n",
    "\n",
    "    while True:\n",
    "        query = input('> ')\n",
    "        if query == \"stop\":\n",
    "            break\n",
    "\n",
    "        output = toia_answer_new(query, dataset)\n",
    "        if output is None:\n",
    "            break\n",
    "\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a0f76aba-77c3-40dc-a8a5-c998b901693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOIA starts\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what's your name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-147-5b3d2fc83f3d>:4: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  return base.similarity(compare)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I don't have an answer for that\", 'r2y', 'spaCy sim: 1.0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  how are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I am fine thanks!', 'ef1', 'tfidf sim: [0.98471278]')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  tell me something I don't know\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I wrote a book about the ethics of artificial intelligence', 'r2c', 'tfidf sim: [0.8660254]')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  wow congrats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"hey, I'm all right\", 'r2m')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what else can I ask you about\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I wrote a book about the ethics of artificial intelligence', 'r2c', 'spaCy sim: 0.7274114971012712')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  ok, got it something else?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"hey, I'm all right\", 'r2m')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what do you do\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I am doing a Ph.D.', 'r2a', 'tfidf sim: [0.9967007]')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  in what subject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Sorry, I didn't get that\", 'r2h', 'spaCy sim: 0.45464181077339555')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  never mind\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I don't have an answer for that\", 'r2y', 'tfidf sim all 0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what is your favorite food\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I love soccer!', '0b2', 'tfidf sim: [0.87910785]')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  stop\n"
     ]
    }
   ],
   "source": [
    "run_toia(df_avatar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db9272-4b20-453e-8bff-2aff711905fd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bca89-98ca-4462-bcc9-7e15de0013f9",
   "metadata": {},
   "source": [
    "## WIP: Can see if an ELIZA-like algorithm can help\n",
    "\n",
    "https://github.com/wadetb/eliza\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211ad7e-149c-462d-a101-ed309b92e538",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run only for the first time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest release of Haystack in your own environment \n",
    "#!pip install git+https://github.com/deepset-ai/haystack.git\n",
    "\n",
    "# If running on GPUs, e.g., DALMA\n",
    "# Install the latest master of Haystack\n",
    "#!pip install git+https://github.com/deepset-ai/haystack.git\n",
    "#!pip install urllib3==1.25.4\n",
    "#!pip install torch==1.6.0+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2021 18:49:02 - INFO - faiss -   Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_store.memory import InMemoryDocumentStore\n",
    "from haystack.retriever.dense import EmbeddingRetriever\n",
    "from haystack.utils import print_answers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-Memory Document Store\n",
    "\n",
    "document_store = InMemoryDocumentStore(similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/13/2021 11:26:42 - INFO - haystack.retriever.dense -   Init retriever using embeddings of model deepset/sentence_bert\n",
      "01/13/2021 11:26:42 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "01/13/2021 11:26:42 - INFO - farm.infer -   Could not find `deepset/sentence_bert` locally. Try to download from model hub ...\n",
      "01/13/2021 11:26:46 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "01/13/2021 11:26:52 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n"
     ]
    }
   ],
   "source": [
    "model_path = \"deepset/sentence_bert\"\n",
    "\n",
    "retriever = EmbeddingRetriever(document_store=document_store, \n",
    "                               embedding_model=model_path, \n",
    "                               use_gpu=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/13/2021 18:45:44 - INFO - haystack.document_store.memory -   Updating embeddings for 352 docs ...\n",
      "/Users/amc/opt/miniconda3/envs/dm_api/lib/python3.7/site-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "Inferencing Samples: 100%|██████████| 88/88 [03:15<00:00,  2.22s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "# Get dataframe with columns \"question\", \"answer\" and some custom metadata\n",
    "df = pd.read_csv(\"data/MargaritaCorpusKB_video_id.csv\")\n",
    "df = df[[\"Context\", \"Utterance\", \"id_video\"]]\n",
    "df = df.rename(columns={\"Context\": \"text\", \"Utterance\": \"answer\"})\n",
    "df.drop_duplicates(subset=['text'], inplace=True)\n",
    "df.drop_duplicates(subset=['answer'], inplace=True)\n",
    "# Minimal cleaning\n",
    "df.fillna(value=\"\", inplace=True)\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: x.strip())\n",
    "# Drop question that only have *\n",
    "index_drop = df[df[\"text\"] == \"*\"].index\n",
    "df.drop(index_drop, inplace=True)\n",
    "\n",
    "# Get embeddings for our questions from the FAQs\n",
    "# questions = list(df[\"text\"].values)\n",
    "# df[\"embedding\"] = retriever.embed_queries(texts=questions)\n",
    "\n",
    "# Convert Dataframe to list of dicts and index them in our DocumentStore\n",
    "docs_to_index = df.to_dict(orient=\"records\")\n",
    "\n",
    "# # Delete existing documents in documents store\n",
    "document_store.delete_all_documents()\n",
    "\n",
    "# Write documents to document store\n",
    "document_store.write_documents(docs_to_index)\n",
    "\n",
    "# Add documents embeddings to index\n",
    "document_store.update_embeddings(\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.74 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not too bad, thanks.\n",
      "ca71d0c3f77e9c0b61f8d810617b3841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_embedding = np.array(\n",
    "    retriever.embed_queries(texts=\"How are you?\")\n",
    ")\n",
    "\n",
    "response = document_store.query_by_embedding(\n",
    "    query_embedding, \n",
    "    top_k=1, \n",
    "    return_embedding=False\n",
    ")\n",
    "\n",
    "print(response[0].meta['answer'])\n",
    "print(response[0].meta['id_video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "outfile = open(\"faiss_indices/margarita.pkl\", 'wb')\n",
    "pickle.dump(document_store, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"faiss_indices/margarita.pkl\",'rb')\n",
    "new_document_store = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not too bad, thanks.\n",
      "ca71d0c3f77e9c0b61f8d810617b3841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query_embedding = np.array(\n",
    "    retriever.embed_queries(texts=\"How are you?\")\n",
    ")\n",
    "response = new_document_store.query_by_embedding(\n",
    "    query_embedding, \n",
    "    top_k=1, \n",
    "    return_embedding=False\n",
    ")\n",
    "\n",
    "print(response[0].meta['answer'])\n",
    "print(response[0].meta['id_video'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialogue Mgr can stop here\n",
    "\n",
    "Below is evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "\n",
    "df_dial = pd.read_csv(\"data/DIALOGUES.csv\")\n",
    "df_dial = df_dial[df_dial['Experiment'] == 'TRAIN']\n",
    "test_questions = df_dial['Q'].to_list()\n",
    "annotation_cols = ['BA1', 'BA2', 'BA3', 'BA4', 'BA5', 'BA6']\n",
    "\n",
    "hits_at_1 = 0\n",
    "hits, probs, scores, answers = [], [], [], []\n",
    "test_questions_emb = retriever.embed_queries(texts=test_questions)\n",
    "for question, embedding in zip(test_questions, test_questions_emb):\n",
    "    prediction = document_store.query_by_embedding(\n",
    "        np.array(embedding), \n",
    "        top_k=1, \n",
    "        return_embedding=False\n",
    "    )\n",
    "    answer = prediction[0].meta['answer']\n",
    "    if answer in df_dial[df_dial['Q'] == question][annotation_cols].values:\n",
    "        hits_at_1 += 1\n",
    "        hits.append(1)\n",
    "    else:\n",
    "        hits_at_1 == 0\n",
    "        hits.append(0)\n",
    "    probs.append(prediction[0].probability)\n",
    "    scores.append(prediction[0].score)\n",
    "    answers.append(answer)\n",
    "\n",
    "hits_at_k = 0\n",
    "for question, embedding in zip(test_questions, test_questions_emb):\n",
    "    predictions = document_store.query_by_embedding(\n",
    "        np.array(embedding), \n",
    "        top_k=10, \n",
    "        return_embedding=False\n",
    "    )\n",
    "    pred_answers = [pred.meta['answer'] for pred in predictions]\n",
    "    if any([pred_ans in df_dial[\n",
    "        df_dial['Q'] == question][annotation_cols].values \n",
    "            for pred_ans in pred_answers]):\n",
    "        hits_at_k += 1\n",
    "    else:\n",
    "        hits_at_k == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4411764705882353\n",
      "0.6058823529411764\n"
     ]
    }
   ],
   "source": [
    "# deepset/sentence_bert on 404 unique q-a pairs\n",
    "print(hits_at_1/len(test_questions))\n",
    "print(hits_at_k/len(test_questions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
